\section{Decision Tree Model}

\subsection{Algorithm Implementation}

\subsubsection{Data Preparation}

Similar to the k-NN model, we use the first 20 features from the dataset to ensure model interpretability. The data is split into training (70\%) and testing (30\%) sets, resulting in 2,295 training samples (310 'ad', 1,985 'nonad') and 984 test samples (149 'ad', 835 'nonad').

\subsubsection{Gini Impurity Function}

We implement the Gini Impurity function as follows:

\begin{lstlisting}[language=R, caption=Gini Impurity Function]
gini_impurity <- function(labels) {
  if(length(labels) == 0) return(0)
  proportions <- table(labels) / length(labels)
  return(1 - sum(proportions^2))
}
\end{lstlisting}

\subsection{Best Split Finding Function}

This function iterates through all attributes and possible thresholds to find the optimal split:

\begin{lstlisting}[language=R, caption=Best Split Finding Function]
find_best_split <- function(data, target_col) {
  best_gini <- Inf
  best_feature <- NULL
  best_threshold <- NULL
  
  for(feature in names(data)[names(data) != target_col]) {
    values <- unique(data[[feature]])
    if(length(values) > 1) {
      for(threshold in values) {
        left_indices <- data[[feature]] <= threshold
        right_indices <- !left_indices
        
        if(sum(left_indices) > 0 && sum(right_indices) > 0) {
          left_gini <- gini_impurity(data[[target_col]][left_indices])
          right_gini <- gini_impurity(data[[target_col]][right_indices])
          
          weighted_gini <- (sum(left_indices) * left_gini + 
                           sum(right_indices) * right_gini) / nrow(data)
          
          if(weighted_gini < best_gini) {
            best_gini <- weighted_gini
            best_feature <- feature
            best_threshold <- threshold
          }
        }
      }
    }
  }
  
  return(list(feature = best_feature, threshold = best_threshold, 
              gini = best_gini))
}
\end{lstlisting}

\subsubsection{Tree Building Function}

The decision tree is built recursively with a maximum depth of 3 to avoid overfitting:

\begin{lstlisting}[language=R, caption=Decision Tree Building Function]
build_simple_tree <- function(data, target_col, max_depth = 3, 
                             current_depth = 0) {
  # Stopping conditions
  if(current_depth >= max_depth || nrow(data) < 10 || 
     length(unique(data[[target_col]])) == 1) {
    majority_class <- names(sort(table(data[[target_col]]), 
                                decreasing = TRUE))[1]
    return(list(type = "leaf", prediction = majority_class, 
                samples = nrow(data)))
  }
  
  # Find best split
  split_info <- find_best_split(data, target_col)
  
  if(is.null(split_info$feature)) {
    majority_class <- names(sort(table(data[[target_col]]), 
                                decreasing = TRUE))[1]
    return(list(type = "leaf", prediction = majority_class, 
                samples = nrow(data)))
  }
  
  # Split data and build subtrees
  left_indices <- data[[split_info$feature]] <= split_info$threshold
  right_indices <- !left_indices
  
  left_data <- data[left_indices, ]
  right_data <- data[right_indices, ]
  
  left_tree <- build_simple_tree(left_data, target_col, 
                                max_depth, current_depth + 1)
  right_tree <- build_simple_tree(right_data, target_col, 
                                 max_depth, current_depth + 1)
  
  return(list(
    type = "node",
    feature = split_info$feature,
    threshold = split_info$threshold,
    left = left_tree,
    right = right_tree,
    samples = nrow(data)
  ))
}
\end{lstlisting}

\subsection{Model Evaluation}

\subsubsection{Training Results}

The decision tree model demonstrated strong performance, achieving a training accuracy of 93.46\% and a test accuracy of 91.06\%. These results, summarized in the table below, indicate that the model generalizes well from the training data to the unseen test data without significant overfitting.

\begin{table}[h]
\centering
\caption{Decision Tree Model Training Results}
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Training Accuracy & 93.46\% \\
Test Accuracy & 91.06\% \\
\hline
\end{tabular}
\end{table}

\subsubsection{Confusion Matrix}

The confusion matrix on the test set shows:

\begin{table}[h]
\centering
\caption{Confusion Matrix - Decision Tree}
\begin{tabular}{|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Predicted}} & \multicolumn{2}{c|}{\textbf{Actual}} \\
\cline{2-3}
 & \textbf{ad} & \textbf{nonad} \\
\hline
\textbf{ad} & 70 & 9 \\
\hline
\textbf{nonad} & 79 & 826 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{graphics/05-dt-confusion_matrix.png}
\caption{Decision Tree Confusion Matrix Visualization}
\end{figure}

\subsubsection{Evaluation Metrics}

Detailed evaluation metrics of the model:

\begin{table}[h]
\centering
\caption{Decision Tree Model Evaluation Metrics}
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Accuracy & 91.06\% \\
Precision (ad) & 88.61\% \\
Recall (ad) & 46.98\% \\
F1-score (ad) & 61.40\% \\
\hline
\end{tabular}
\end{table}

\subsection{Decision Tree Structure}

The constructed decision tree has the following structure:

\begin{figure}[h]
\centering
\begin{verbatim}
Node: X1 <= 389 (samples: 2295)
|-- Left:
    Node: X11 <= 0 (samples: 2103)
    |-- Left:
        Node: X9 <= 0 (samples: 2092)
        |-- Left:
            Leaf: Predict nonad (samples: 2077)
        |-- Right:
            Leaf: Predict ad (samples: 15)
    |-- Right:
        Leaf: Predict ad (samples: 11)
|-- Right:
    Node: X0 <= 20 (samples: 192)
    |-- Left:
        Leaf: Predict nonad (samples: 22)
    |-- Right:
        Node: X2 <= 5.05 (samples: 170)
        |-- Left:
            Leaf: Predict nonad (samples: 12)
        |-- Right:
            Leaf: Predict ad (samples: 158)
\end{verbatim}
\caption{Decision tree structure with maximum depth of 3}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{graphics/05-dt-feature_importance.png}
\caption{Decision Tree Feature Importance}
\end{figure}

\subsection{Results Analysis}

\subsubsection{Model Strengths}

\begin{itemize}
    \item \textbf{High interpretability}: Decision trees provide clear and understandable rules, allowing users to follow the decision-making process.
    \item \textbf{High accuracy}: The model achieves 91.06\% accuracy on the test set, higher than the k-NN model (81\%).
    \item \textbf{High precision}: With 88.61\% precision for the "ad" class, the model can accurately identify advertisements.
    \item \textbf{No data normalization required}: Unlike k-NN, decision trees are not affected by the scale of features.
\end{itemize}

\subsubsection{Model Limitations}

\begin{itemize}
    \item \textbf{Low recall}: With only 46.98\% recall, the model misses many actual advertisement cases.
    \item \textbf{Prone to overfitting}: Despite depth limitations, decision trees still tend to memorize training data.
    \item \textbf{Instability}: Small changes in data can lead to completely different trees.
    \item \textbf{Bias towards features with many values}: The algorithm may favor features with many distinct values.
\end{itemize}

\subsubsection{Impact of Class Imbalance}

Similar to k-NN, the decision tree model is also affected by class imbalance in the data ("nonad":"ad" ratio is approximately 5.6:1). This leads to:

\begin{itemize}
    \item The model tends to predict the "nonad" class more often
    \item Low recall for the "ad" class (46.98\%)
    \item Moderate F1-score (61.40\%) due to the balance between precision and recall
\end{itemize}

\subsection{Conclusion}

The Decision Tree model achieved strong performance in Internet advertisement classification with 91.06\% accuracy on the test set, the highest among the models tested. The model demonstrated high precision of 88.61\% and moderate recall of 46.98\% for advertisement detection, resulting in an F1-score of 61.4\%. 

Key findings from the implementation:
\begin{itemize}
    \item Excellent overall accuracy (91.06\%) with strong generalization capability
    \item High precision (88.61\%) indicates reliable advertisement detection with minimal false positives
    \item The tree structure with max depth of 3 provides optimal balance between performance and interpretability
    \item Features X1, X11, X9, X0, and X2 emerged as the most critical decision variables
    \item Using only 20 selected features achieved better performance than high-dimensional approaches
\end{itemize}

The Decision Tree's superior accuracy combined with its interpretable structure makes it highly suitable for advertisement classification where both performance and explainability are important requirements. The tree structure reveals that feature X1 serves as the root node with threshold 389, demonstrating its critical importance in distinguishing between advertisements and non-advertisements.

Future improvements could focus on addressing class imbalance through techniques such as SMOTE or cost-sensitive learning to enhance recall performance while maintaining the model's high precision.