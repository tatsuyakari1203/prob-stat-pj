% 'draft' mode can be used to speed up compilation
\documentclass[twoside,final]{hcmut-report}
\usepackage{codespace}

% Draft watermark
% https://github.com/callegar/LaTeX-draftwatermark

% Encodings
\usepackage{gensymb,textcomp}

% Better tables
% Wide tables go to https://tex.stackexchange.com/q/332902
\usepackage{array,longtable,multicol,multirow,siunitx,tabularx}

% Better enum
\usepackage{enumitem}

% Graphics
\usepackage{caption,float}

% Add options for figures, like max width, framing, etc.
\usepackage[export]{adjustbox}

% References
% Use \Cref{} instead of \ref{}
\usepackage[nameinlink]{cleveref}

% FOR DEMONSTRATION PURPOSES, REMOVE IN PRODUCTION
\usepackage{mwe}

% Sub-preambles
% https://github.com/MartinScharrer/standalone

% Configurations
\coursename{Probability and Statistics}
\reporttype{Final Project Report}
\title{Internet Advertisement Classification using Random Forest}
\advisor{& [Instructor Name] &}
\stuname{%
  & [Student Name 1] & [Student ID 1] \\
  & [Student Name 2] & [Student ID 2] \\
  & [Student Name 3] & [Student ID 3] \\
}

% Allow page breaks inside align* environment
%\allowdisplaybreaks{}

% Makes a lot of things blue, avoid at all costs
%\everymath{\color{blue}}

% Set depth of numbering for counters
\AtBeginDocument{\counterwithin{lstlisting}{section}}

% Rename some sections
%\AtBeginDocument{\renewcommand*{\contentsname}{Contents}}
%\AtBeginDocument{\renewcommand*{\refname}{References}}
%\AtBeginDocument{\renewcommand*{\bibname}{References}}

% Custom commands
%\newcommand*\mean[1]{\bar{#1}}

\begin{document}
\coverpage%

%\section*{Member list \& Workload}
%\newcounter{memberrowno}
%\setcounter{memberrowno}{0}
%\begin{center}
%  \begin{tabular}{>{\stepcounter{memberrowno}\thememberrowno}llcc}
%    \toprule
%    \multicolumn{1}{c}{\textbf{No.}} & \textbf{Full name} & \textbf{Student ID} & \textbf{Contribution} \\
%    \midrule
%                                     & h                  & xxxxxxx             & 100\%                       \\
%                                     & h                  & xxxxxxx             & 100\%                       \\
%    \bottomrule
%  \end{tabular}
%\end{center}
%\clearpage

\tableofcontents
\listoffigures
\listoftables
\lstlistoflistings{}

\clearpage

\section{Introduction}
\label{sec:introduction}

Internet advertisement classification is a crucial problem in the field of data processing and machine learning. With the rapid development of the Internet, automatic recognition and classification of advertisements helps improve user experience and optimize content display.

This report presents the analysis process of the "Internet Advertisements Data Set" from the UCI Machine Learning Repository \cite{lichman2013uci} to build a classification model aimed at predicting whether an image is an advertisement or not.

\section{Data Description}
\label{sec:data-description}

The "Internet Advertisements" dataset consists of 3279 samples with 1559 columns. Each row in the data represents an image labeled as "ad" (advertisement) or "nonad" (non-advertisement) in the last column. Columns 0 to 1557 represent numerical attributes of the image.

\subsection{Dataset Characteristics}
\begin{itemize}
  \item Number of samples: 3279
  \item Number of features: 1558
  \item Target variable: binary (ad/nonad)
  \item Missing values: represented by "?"
\end{itemize}

\section{Data Cleaning}
\label{sec:data-cleaning}

The data cleaning process includes the following steps:

\begin{enumerate}
  \item \textbf{Handling missing values}: Replace "?" values with the median of each column
  \item \textbf{Standardizing target variable}: Ensure class labels have consistent format
  \item \textbf{Removing index columns}: Delete unnecessary index columns for classification
\end{enumerate}

\begin{lstlisting}[language=R, caption={R code for data cleaning}, label={lst:data-cleaning}]
# Handle missing values
for(i in 1:(ncol(data)-1)) {
  if(any(data[[i]] == "?")) {
    data[[i]] <- as.numeric(ifelse(data[[i]] == "?", NA, data[[i]]))
    data[[i]][is.na(data[[i]])] <- median(data[[i]], na.rm = TRUE)
  }
}
\end{lstlisting}

\section{Descriptive Statistics}
\label{sec:descriptive-statistics}

Descriptive statistical analysis was performed to better understand the data distribution and relationships between variables.

\subsection{Target Variable Distribution}
The dataset has class imbalance with approximately 1:6 ratio between "ad" and "nonad" classes.

\subsection{Statistical Plots}
\begin{itemize}
  \item \textbf{Histogram}: Display feature distributions
  \item \textbf{Boxplot}: Detect outliers
  \item \textbf{Scatter plot}: Explore relationships between variables
  \item \textbf{Correlation heatmap}: Display correlation matrix
\end{itemize}

\section{Objective and Methodology}
\label{sec:objective-methodology}

\subsection{Objective}
Build a classification model to accurately predict whether an image is an advertisement or not based on numerical features.

\subsection{Statistical Methods Used}
The project uses three main machine learning methods:

\begin{enumerate}
  \item \textbf{k-Nearest Neighbors (k-NN)} \cite{cover1967nearest}: Simple distance-based method
  \item \textbf{Decision Tree} \cite{quinlan1993c4}: Interpretable model with tree structure
  \item \textbf{Random Forest} \cite{breiman2001random}: Ensemble method combining multiple decision trees
\end{enumerate}

\textbf{Reasons for choosing Random Forest as the main method}:
\begin{itemize}
  \item Handles high-dimensional data well
  \item Reduces overfitting compared to single Decision Trees
  \item Provides feature importance information
  \item High performance on various types of data
\end{itemize}

\section{Main Results}
\label{sec:main-results}

\subsection{Model Performance}

\begin{table}[H]
\centering
\caption{Model performance comparison}
\label{tab:model-comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} \\
\midrule
k-NN (k=3) & 0.81 & 0.99 & 0.75 \\
Decision Tree & 0.91 & 0.89 & 0.47 \\
Random Forest & 0.91 & 1.00 & 0.38 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Random Forest Confusion Matrix}

\begin{lstlisting}[language=R, caption={R code for Random Forest}, label={lst:random-forest}]
# Build Random Forest with 100 trees
rf_model <- randomForest(x = X_train, y = y_train, 
                        ntree = 100, 
                        mtry = floor(sqrt(ncol(X_train))))

# Predict on test set
y_pred <- predict(rf_model, X_test)
\end{lstlisting}

\subsection{Feature Importance}
Random Forest shows the most important features in advertisement classification, with X2, X1243, and X1 being the three features with the greatest impact.

\subsection{Results Analysis}
\begin{itemize}
  \item \textbf{Random Forest} achieved the highest accuracy (91\%) with perfect precision (100\%)
  \item \textbf{Decision Tree} provides good balance between performance and interpretability
  \item \textbf{k-NN} shows stable performance with simple approach
  \item All models handle the classification problem well with accuracy above 80\%
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

The project has successfully applied machine learning methods to classify Internet advertisements. Random Forest was identified as the best method with 91\% accuracy and perfect precision, suitable for deployment in real advertisement filtering systems.

Using Random Forest - a method not previously learned in class - has provided deep insights into handling high-dimensional data and the importance of combining multiple models to improve performance.

\subsection{Member Contributions}
\begin{itemize}
  \item [Member 1]: Data preprocessing and descriptive statistical analysis
  \item [Member 2]: Building and evaluating machine learning models
  \item [Member 3]: Report writing and result presentation
\end{itemize}

\clearpage

\bibliographystyle{plain}
\bibliography{refs/example.bib}
\nocite{*}

\end{document}
